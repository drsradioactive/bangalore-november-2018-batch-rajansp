{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84Q8JfvaeZZ6"
   },
   "source": [
    "## Linear Classifier in TensorFlow \n",
    "Using Low Level API in Eager Execution mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHpCNRv1OB5-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mjtb-EMcm5K0"
   },
   "outputs": [],
   "source": [
    "#Enable Eager Execution if using tensflow version < 2.0\n",
    "#From tensorflow v2.0 onwards, Eager Execution will be enabled by default\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxJDmJqqOB6K",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhllFLyKOB6N",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgkX6SEqOB6W"
   },
   "source": [
    "### Check all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7K8pWsNQOB6X"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-05 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-06 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-07 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-08 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date symbol        open       close         low        high  \\\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000  125.839996  122.309998  126.250000   \n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002  125.540001   \n",
       "2  2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.930000  119.739998   \n",
       "3  2016-01-08 00:00:00   WLTW  115.480003  116.620003  113.500000  117.440002   \n",
       "4  2016-01-11 00:00:00   WLTW  117.010002  114.970001  114.089996  117.330002   \n",
       "\n",
       "      volume  \n",
       "0  2163600.0  \n",
       "1  2386400.0  \n",
       "2  2489500.0  \n",
       "3  2006300.0  \n",
       "4  1408600.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>851259</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZBH</td>\n",
       "      <td>103.309998</td>\n",
       "      <td>103.199997</td>\n",
       "      <td>102.849998</td>\n",
       "      <td>103.930000</td>\n",
       "      <td>973800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851260</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>43.070000</td>\n",
       "      <td>43.040001</td>\n",
       "      <td>42.689999</td>\n",
       "      <td>43.310001</td>\n",
       "      <td>1938100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851261</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>53.639999</td>\n",
       "      <td>53.529999</td>\n",
       "      <td>53.270000</td>\n",
       "      <td>53.740002</td>\n",
       "      <td>1701200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851262</th>\n",
       "      <td>2016-12-30 00:00:00</td>\n",
       "      <td>AIV</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>45.450001</td>\n",
       "      <td>44.410000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>1380900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851263</th>\n",
       "      <td>2016-12-30 00:00:00</td>\n",
       "      <td>FTV</td>\n",
       "      <td>54.200001</td>\n",
       "      <td>53.630001</td>\n",
       "      <td>53.389999</td>\n",
       "      <td>54.480000</td>\n",
       "      <td>705100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date symbol        open       close         low  \\\n",
       "851259           2016-12-30    ZBH  103.309998  103.199997  102.849998   \n",
       "851260           2016-12-30   ZION   43.070000   43.040001   42.689999   \n",
       "851261           2016-12-30    ZTS   53.639999   53.529999   53.270000   \n",
       "851262  2016-12-30 00:00:00    AIV   44.730000   45.450001   44.410000   \n",
       "851263  2016-12-30 00:00:00    FTV   54.200001   53.630001   53.389999   \n",
       "\n",
       "              high     volume  \n",
       "851259  103.930000   973800.0  \n",
       "851260   43.310001  1938100.0  \n",
       "851261   53.740002  1701200.0  \n",
       "851262   45.590000  1380900.0  \n",
       "851263   54.480000   705100.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(851264, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "date       object\n",
       "symbol     object\n",
       "open      float64\n",
       "close     float64\n",
       "low       float64\n",
       "high      float64\n",
       "volume    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>851264.0</td>\n",
       "      <td>7.083699e+01</td>\n",
       "      <td>8.369588e+01</td>\n",
       "      <td>0.85</td>\n",
       "      <td>3.384000e+01</td>\n",
       "      <td>5.277000e+01</td>\n",
       "      <td>7.988000e+01</td>\n",
       "      <td>1.584440e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close</th>\n",
       "      <td>851264.0</td>\n",
       "      <td>7.085711e+01</td>\n",
       "      <td>8.368969e+01</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.385000e+01</td>\n",
       "      <td>5.280000e+01</td>\n",
       "      <td>7.989000e+01</td>\n",
       "      <td>1.578130e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>851264.0</td>\n",
       "      <td>7.011841e+01</td>\n",
       "      <td>8.287729e+01</td>\n",
       "      <td>0.83</td>\n",
       "      <td>3.348000e+01</td>\n",
       "      <td>5.223000e+01</td>\n",
       "      <td>7.911000e+01</td>\n",
       "      <td>1.549940e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>851264.0</td>\n",
       "      <td>7.154348e+01</td>\n",
       "      <td>8.446550e+01</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.419000e+01</td>\n",
       "      <td>5.331000e+01</td>\n",
       "      <td>8.061000e+01</td>\n",
       "      <td>1.600930e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume</th>\n",
       "      <td>851264.0</td>\n",
       "      <td>5.415113e+06</td>\n",
       "      <td>1.249468e+07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.221500e+06</td>\n",
       "      <td>2.476250e+06</td>\n",
       "      <td>5.222500e+06</td>\n",
       "      <td>8.596434e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count          mean           std   min           25%  \\\n",
       "open    851264.0  7.083699e+01  8.369588e+01  0.85  3.384000e+01   \n",
       "close   851264.0  7.085711e+01  8.368969e+01  0.86  3.385000e+01   \n",
       "low     851264.0  7.011841e+01  8.287729e+01  0.83  3.348000e+01   \n",
       "high    851264.0  7.154348e+01  8.446550e+01  0.88  3.419000e+01   \n",
       "volume  851264.0  5.415113e+06  1.249468e+07  0.00  1.221500e+06   \n",
       "\n",
       "                 50%           75%           max  \n",
       "open    5.277000e+01  7.988000e+01  1.584440e+03  \n",
       "close   5.280000e+01  7.989000e+01  1.578130e+03  \n",
       "low     5.223000e+01  7.911000e+01  1.549940e+03  \n",
       "high    5.331000e+01  8.061000e+01  1.600930e+03  \n",
       "volume  2.476250e+06  5.222500e+06  8.596434e+08  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                        date symbol        open       close         low  \\\n",
       "0       2016-01-05 00:00:00   WLTW  123.430000  125.839996  122.309998   \n",
       "1       2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002   \n",
       "2       2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.930000   \n",
       "3       2016-01-08 00:00:00   WLTW  115.480003  116.620003  113.500000   \n",
       "4       2016-01-11 00:00:00   WLTW  117.010002  114.970001  114.089996   \n",
       "5       2016-01-12 00:00:00   WLTW  115.510002  115.550003  114.500000   \n",
       "6       2016-01-13 00:00:00   WLTW  116.459999  112.849998  112.589996   \n",
       "7       2016-01-14 00:00:00   WLTW  113.510002  114.379997  110.050003   \n",
       "8       2016-01-15 00:00:00   WLTW  113.330002  112.529999  111.919998   \n",
       "9       2016-01-19 00:00:00   WLTW  113.660004  110.379997  109.870003   \n",
       "10      2016-01-20 00:00:00   WLTW  109.059998  109.300003  108.320000   \n",
       "11      2016-01-21 00:00:00   WLTW  109.730003  110.000000  108.320000   \n",
       "12      2016-01-22 00:00:00   WLTW  111.879997  111.949997  110.190002   \n",
       "13      2016-01-25 00:00:00   WLTW  111.320000  110.120003  110.000000   \n",
       "14      2016-01-26 00:00:00   WLTW  110.419998  111.000000  107.300003   \n",
       "15      2016-01-27 00:00:00   WLTW  110.769997  110.709999  109.019997   \n",
       "16      2016-01-28 00:00:00   WLTW  110.900002  112.580002  109.900002   \n",
       "17      2016-01-29 00:00:00   WLTW  113.349998  114.470001  111.669998   \n",
       "18      2016-02-01 00:00:00   WLTW  114.000000  114.500000  112.900002   \n",
       "19      2016-02-02 00:00:00   WLTW  113.250000  110.559998  109.750000   \n",
       "20      2016-02-03 00:00:00   WLTW  113.379997  114.050003  109.639999   \n",
       "21      2016-02-04 00:00:00   WLTW  114.080002  115.709999  114.080002   \n",
       "22      2016-02-05 00:00:00   WLTW  115.120003  114.019997  109.709999   \n",
       "23      2016-02-08 00:00:00   WLTW  113.300003  111.160004  110.459999   \n",
       "24      2016-02-09 00:00:00   WLTW  111.169998  110.650002  109.639999   \n",
       "25      2016-02-10 00:00:00   WLTW  106.730003  107.519997  106.360001   \n",
       "26      2016-02-11 00:00:00   WLTW  105.629997  107.129997  104.110001   \n",
       "27      2016-02-12 00:00:00   WLTW  108.559998  107.839996  107.070000   \n",
       "28      2016-02-16 00:00:00   WLTW  109.110001  110.769997  107.010002   \n",
       "29      2016-02-17 00:00:00   WLTW  110.830002  111.239998  107.970001   \n",
       "...                     ...    ...         ...         ...         ...   \n",
       "851234           2016-12-30    WAT  135.240005  134.389999  133.710007   \n",
       "851235           2016-12-30    WBA   83.459999   82.760002   82.419998   \n",
       "851236           2016-12-30    WDC   68.550003   67.949997   67.610001   \n",
       "851237           2016-12-30    WEC   58.980000   58.650002   58.419998   \n",
       "851238           2016-12-30    WFC   54.889999   55.110001   54.790001   \n",
       "851239           2016-12-30    WFM   31.059999   30.760000   30.670000   \n",
       "851240           2016-12-30    WHR  183.800003  181.770004  180.869995   \n",
       "851241           2016-12-30     WM   71.269997   70.910004   70.750000   \n",
       "851242           2016-12-30    WMB   30.940001   31.139999   30.889999   \n",
       "851243           2016-12-30    WMT   69.120003   69.120003   68.830002   \n",
       "851244           2016-12-30    WRK   51.840000   50.770000   50.529999   \n",
       "851245           2016-12-30     WU   21.840000   21.719999   21.600000   \n",
       "851246           2016-12-30     WY   30.450001   30.090000   29.950001   \n",
       "851247           2016-12-30    WYN   76.849998   76.370003   76.180000   \n",
       "851248           2016-12-30   WYNN   87.099998   86.510002   85.570000   \n",
       "851249           2016-12-30    XEC  136.520004  135.899994  135.309998   \n",
       "851250           2016-12-30    XEL   41.000000   40.700001   40.560001   \n",
       "851251           2016-12-30     XL   37.360001   37.259998   37.060001   \n",
       "851252           2016-12-30   XLNX   61.090000   60.369999   60.020000   \n",
       "851253           2016-12-30    XOM   90.029999   90.260002   90.010002   \n",
       "851254           2016-12-30   XRAY   58.290001   57.730000   57.540001   \n",
       "851255           2016-12-30    XRX    8.720000    8.730000    8.700000   \n",
       "851256           2016-12-30    XYL   49.980000   49.520000   49.360001   \n",
       "851257           2016-12-30   YHOO   38.720001   38.669998   38.430000   \n",
       "851258           2016-12-30    YUM   63.930000   63.330002   63.160000   \n",
       "851259           2016-12-30    ZBH  103.309998  103.199997  102.849998   \n",
       "851260           2016-12-30   ZION   43.070000   43.040001   42.689999   \n",
       "851261           2016-12-30    ZTS   53.639999   53.529999   53.270000   \n",
       "851262  2016-12-30 00:00:00    AIV   44.730000   45.450001   44.410000   \n",
       "851263  2016-12-30 00:00:00    FTV   54.200001   53.630001   53.389999   \n",
       "\n",
       "              high      volume  \n",
       "0       126.250000   2163600.0  \n",
       "1       125.540001   2386400.0  \n",
       "2       119.739998   2489500.0  \n",
       "3       117.440002   2006300.0  \n",
       "4       117.330002   1408600.0  \n",
       "5       116.059998   1098000.0  \n",
       "6       117.070000    949600.0  \n",
       "7       115.029999    785300.0  \n",
       "8       114.879997   1093700.0  \n",
       "9       115.870003   1523500.0  \n",
       "10      111.599998   1653900.0  \n",
       "11      110.580002    944300.0  \n",
       "12      112.949997    744900.0  \n",
       "13      114.629997    703800.0  \n",
       "14      111.400002    563100.0  \n",
       "15      112.570000    896100.0  \n",
       "16      112.970001    680400.0  \n",
       "17      114.589996    749900.0  \n",
       "18      114.849998    574200.0  \n",
       "19      113.860001    694800.0  \n",
       "20      114.639999    896300.0  \n",
       "21      116.320000    956300.0  \n",
       "22      116.489998    997100.0  \n",
       "23      113.300003   1200500.0  \n",
       "24      112.110001   1725200.0  \n",
       "25      112.110001   1946000.0  \n",
       "26      109.260002   1319500.0  \n",
       "27      109.430000    922400.0  \n",
       "28      111.300003   1185100.0  \n",
       "29      112.110001    921500.0  \n",
       "...            ...         ...  \n",
       "851234  135.300003    464200.0  \n",
       "851235   83.620003   3343200.0  \n",
       "851236   69.400002   2824100.0  \n",
       "851237   59.119999   1221800.0  \n",
       "851238   55.360001  15095500.0  \n",
       "851239   31.299999   2707500.0  \n",
       "851240  184.289993    458200.0  \n",
       "851241   71.500000   1230600.0  \n",
       "851242   31.650000   3980300.0  \n",
       "851243   69.430000   6872000.0  \n",
       "851244   51.840000    811200.0  \n",
       "851245   21.900000   2538900.0  \n",
       "851246   30.450001   2825300.0  \n",
       "851247   76.970001    524600.0  \n",
       "851248   87.449997   1888500.0  \n",
       "851249  137.559998    466100.0  \n",
       "851250   41.070000   1887600.0  \n",
       "851251   37.419998    959200.0  \n",
       "851252   61.480000   2111700.0  \n",
       "851253   90.699997   9117800.0  \n",
       "851254   58.360001    949200.0  \n",
       "851255    8.800000  11250400.0  \n",
       "851256   50.000000    646200.0  \n",
       "851257   39.000000   6431600.0  \n",
       "851258   63.939999   1887100.0  \n",
       "851259  103.930000    973800.0  \n",
       "851260   43.310001   1938100.0  \n",
       "851261   53.740002   1701200.0  \n",
       "851262   45.590000   1380900.0  \n",
       "851263   54.480000    705100.0  \n",
       "\n",
       "[851264 rows x 7 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>851264.000000</td>\n",
       "      <td>851264.000000</td>\n",
       "      <td>851264.000000</td>\n",
       "      <td>851264.000000</td>\n",
       "      <td>8.512640e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>70.836986</td>\n",
       "      <td>70.857109</td>\n",
       "      <td>70.118414</td>\n",
       "      <td>71.543476</td>\n",
       "      <td>5.415113e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>83.695876</td>\n",
       "      <td>83.689686</td>\n",
       "      <td>82.877294</td>\n",
       "      <td>84.465504</td>\n",
       "      <td>1.249468e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.840000</td>\n",
       "      <td>33.849998</td>\n",
       "      <td>33.480000</td>\n",
       "      <td>34.189999</td>\n",
       "      <td>1.221500e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.770000</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>52.230000</td>\n",
       "      <td>53.310001</td>\n",
       "      <td>2.476250e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79.879997</td>\n",
       "      <td>79.889999</td>\n",
       "      <td>79.110001</td>\n",
       "      <td>80.610001</td>\n",
       "      <td>5.222500e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1584.439941</td>\n",
       "      <td>1578.130005</td>\n",
       "      <td>1549.939941</td>\n",
       "      <td>1600.930054</td>\n",
       "      <td>8.596434e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open          close            low           high  \\\n",
       "count  851264.000000  851264.000000  851264.000000  851264.000000   \n",
       "mean       70.836986      70.857109      70.118414      71.543476   \n",
       "std        83.695876      83.689686      82.877294      84.465504   \n",
       "min         0.850000       0.860000       0.830000       0.880000   \n",
       "25%        33.840000      33.849998      33.480000      34.189999   \n",
       "50%        52.770000      52.799999      52.230000      53.310001   \n",
       "75%        79.879997      79.889999      79.110001      80.610001   \n",
       "max      1584.439941    1578.130005    1549.939941    1600.930054   \n",
       "\n",
       "             volume  \n",
       "count  8.512640e+05  \n",
       "mean   5.415113e+06  \n",
       "std    1.249468e+07  \n",
       "min    0.000000e+00  \n",
       "25%    1.221500e+06  \n",
       "50%    2.476250e+06  \n",
       "75%    5.222500e+06  \n",
       "max    8.596434e+08  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.tail()\n",
    "df.shape\n",
    "df.dtypes\n",
    "df.describe().transpose()\n",
    "df.info\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dU6X7MpOB6c"
   },
   "source": [
    "### Drop columns `date` and  `symbol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh_6spSKOB6e"
   },
   "outputs": [],
   "source": [
    "df= df.drop(labels = \"date\", axis = 1)\n",
    "df= df.drop(labels = \"symbol\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlwbUgTwOB6i",
    "outputId": "56bad82a-f271-415a-e0d6-cbe1c4290743"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DBv3WWYOB6q"
   },
   "source": [
    "### Consider only first 1000 rows in the dataset for building feature set and target set\n",
    "Target 'Volume' has very high values. Divide 'Volume' by 1000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_hG9rGBOB6s"
   },
   "outputs": [],
   "source": [
    "df=df.iloc[0:1000,:]\n",
    "df[\"volume\"] = df[\"volume\"]/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "open      float64\n",
       "close     float64\n",
       "low       float64\n",
       "high      float64\n",
       "volume    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high  volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163.6\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386.4\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489.5\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006.3\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408.6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.dtypes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3UaApqYOB6x"
   },
   "source": [
    "### Divide the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,0:4]\n",
    "Y = df['volume']\n",
    "X.shape\n",
    "Y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LE4U8lTdQJq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYK-aUuLbrz2"
   },
   "source": [
    "#### Convert Training and Test Data to numpy float32 arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ao-S0tQGcncz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "train_x = np.float32(X_train)\n",
    "print(train_x.dtype)\n",
    "train_y = np.float32(y_train)\n",
    "print(train_y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "test_x = np.float32(X_test)\n",
    "print(test_x.dtype)\n",
    "test_y = np.float32(y_test)\n",
    "print(test_y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "im1ZegbDdKgv"
   },
   "source": [
    "### Normalize the data\n",
    "You can use Normalizer from sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "train_x_fit = sc.fit(train_x)\n",
    "train_x = train_x_fit.transform(train_x)\n",
    "test_x_fit = sc.fit(test_x)\n",
    "test_x_std = test_x_fit.transform(test_x)\n",
    "\n",
    "test_x_std.shape\n",
    "test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6vE4eYCOB62",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building the Model in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "297_qja4OB7A",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1.Define Weights and Bias, use tf.zeros to initialize weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare Weights and Bias\n",
    "\n",
    "w = tf.zeros(shape=(4,1))\n",
    "b = tf.zeros(shape=(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgtWA-UIOB7F",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2.Define a function to calculate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# y = wx + b\n",
    "\n",
    "def prediction(x, w, b):\n",
    "    xw_matmul = tf.matmul(x, w)\n",
    "    y = tf.add(xw_matmul, b)\n",
    "    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL1hIwf_OB7M",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3.Loss (Cost) Function [Mean square error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Loss\n",
    "\n",
    "def loss(y_actual, y_predicted):\n",
    "    \n",
    "    diff = y_actual - y_predicted\n",
    "    sqr = tf.square(diff)\n",
    "    avg = tf.reduce_mean(sqr)\n",
    "    \n",
    "    return avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzG85FUlOB7U",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4.Function to train the Model\n",
    "\n",
    "1.   Record all the mathematical steps to calculate Loss\n",
    "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
    "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Gradient Descent function\n",
    "\n",
    "def train(x, y_actual, w, b, learning_rate=0.01):\n",
    "    \n",
    "    #Record math ops on 'tape' to calculate loss\n",
    "    with tf.GradientTape() as t:\n",
    "        \n",
    "        t.watch([w,b])\n",
    "        \n",
    "        current_prediction = prediction(x, w, b)\n",
    "        current_loss = loss(y_actual, current_prediction)\n",
    "    \n",
    "    #Calculate Gradients for Loss w.r.t Weights and Bias\n",
    "    dw, db = t.gradient(current_loss,[w, b])\n",
    "    \n",
    "    #Update Weights and Bias\n",
    "    w = w - learning_rate*dw\n",
    "    b = b - learning_rate*db\n",
    "    \n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSypb_u8OB7e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train the model for 100 epochs \n",
    "1. Observe the training loss at every iteration\n",
    "2. Observe Test loss at every 5th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss on iteration 0 308311140.0\n",
      "Current Loss on iteration 5 302536600.0\n",
      "Current Loss on iteration 10 297818620.0\n",
      "Current Loss on iteration 15 293963460.0\n",
      "Current Loss on iteration 20 290813630.0\n",
      "Current Loss on iteration 25 288240100.0\n",
      "Current Loss on iteration 30 286137200.0\n",
      "Current Loss on iteration 35 284418980.0\n",
      "Current Loss on iteration 40 283015140.0\n",
      "Current Loss on iteration 45 281867970.0\n",
      "Current Loss on iteration 50 280930780.0\n",
      "Current Loss on iteration 55 280165060.0\n",
      "Current Loss on iteration 60 279539230.0\n",
      "Current Loss on iteration 65 279028030.0\n",
      "Current Loss on iteration 70 278610340.0\n",
      "Current Loss on iteration 75 278268830.0\n",
      "Current Loss on iteration 80 277990200.0\n",
      "Current Loss on iteration 85 277762270.0\n",
      "Current Loss on iteration 90 277576060.0\n",
      "Current Loss on iteration 95 277423870.0\n"
     ]
    }
   ],
   "source": [
    "#Training the model for 100 iterations\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    w, b = train(train_x, train_y, w, b)\n",
    "    if i % 5 == 0:\n",
    "        print('Current Loss on iteration', i, loss(train_y, prediction(train_x, w, b)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOL2ncA1OB7q"
   },
   "source": [
    "### Get the shapes and values of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGvtyTeuOB7r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(4), Dimension(1)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4483, shape=(4, 1), dtype=float32, numpy=\n",
       "array([[ 1.9218445e-04],\n",
       "       [-1.7013549e-04],\n",
       "       [-4.8542021e-05],\n",
       "       [ 1.0871886e-06]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w.shape\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhDtOv5UOB7x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4486, shape=(1,), dtype=float32, numpy=array([4972.7715], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERq9GOKKciho"
   },
   "source": [
    "### Model Prediction on 1st Examples in Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4483, shape=(4, 1), dtype=float32, numpy=\n",
       "array([[ 1.9218445e-04],\n",
       "       [-1.7013549e-04],\n",
       "       [-4.8542021e-05],\n",
       "       [ 1.0871886e-06]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4486, shape=(1,), dtype=float32, numpy=array([4972.7715], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4520, shape=(1,), dtype=float32, numpy=array([4972.7695], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_prediction = prediction(test_x, w, b)\n",
    "\n",
    "current_prediction[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJRBuqXhOB7_"
   },
   "source": [
    "## Classification using tf.Keras\n",
    "\n",
    "In this exercise, we will build a Deep Neural Network using tf.Keras. We will use Iris Dataset for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow if not done already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0g6lorycihf"
   },
   "source": [
    "### Load the given Iris data using pandas (Iris.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xFvb5sRcihg"
   },
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('Iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      "Id               150 non-null int64\n",
      "SepalLengthCm    150 non-null float64\n",
      "SepalWidthCm     150 non-null float64\n",
      "PetalLengthCm    150 non-null float64\n",
      "PetalWidthCm     150 non-null float64\n",
      "Species          150 non-null object\n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 7.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.info()\n",
    "iris_df.shape\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAB--Qdwcihm"
   },
   "source": [
    "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D95nY5ILcihj"
   },
   "source": [
    "### Splitting the data into feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RyMQoLMucihj"
   },
   "outputs": [],
   "source": [
    "X = iris_df.iloc[:,1:5].values\n",
    "y = iris_df.iloc[:,5].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder =  LabelEncoder()\n",
    "y1 = encoder.fit_transform(y)\n",
    "\n",
    "Y = pd.get_dummies(y1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERq9GOKKciho"
   },
   "source": [
    "### Divide the dataset into Training and test (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKGvUWahcihp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train  (105, 4)\n",
      "X_test  (45, 4)\n",
      "Y_train  (105, 3)\n",
      "Y_test  (45, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=1)\n",
    "print(\"X_train \", X_train.shape)\n",
    "print(\"X_test \", X_test.shape)\n",
    "print(\"Y_train \", Y_train.shape)\n",
    "print(\"Y_test \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b22qpC5xcihr"
   },
   "source": [
    "###  Building Model in tf.keras\n",
    "\n",
    "Build a Linear Classifier model  <br>\n",
    "1.  Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3<br> \n",
    "2. Apply Softmax on Dense Layer outputs <br>\n",
    "3. Use SGD as Optimizer\n",
    "4. Use categorical_crossentropy as loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hov_UFnUciht"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qLEdHPscihx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 21        \n",
      "=================================================================\n",
      "Total params: 213\n",
      "Trainable params: 213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10,input_shape=(4,),activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(6,activation='relu'))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "#model.compile(Adam(lr=0.04),'categorical_crossentropy',metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5FdzqIKcihw"
   },
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBgKZkhkcih6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7874 - acc: 0.6952\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7821 - acc: 0.6952\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7759 - acc: 0.6952\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7704 - acc: 0.6952\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7648 - acc: 0.6952\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 38us/step - loss: 0.7606 - acc: 0.6952\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7550 - acc: 0.6952\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.7511 - acc: 0.6952\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7457 - acc: 0.6952\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7415 - acc: 0.6952\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.7363 - acc: 0.6952\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7322 - acc: 0.6952\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.7285 - acc: 0.6952\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.7249 - acc: 0.6952\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7198 - acc: 0.6952\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7159 - acc: 0.6952\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7124 - acc: 0.6952\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 38us/step - loss: 0.7088 - acc: 0.6952\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7046 - acc: 0.6952\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.7008 - acc: 0.6952\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6975 - acc: 0.6952\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6940 - acc: 0.6952\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6912 - acc: 0.6952\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6874 - acc: 0.6952\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6841 - acc: 0.6952\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6811 - acc: 0.6952\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6785 - acc: 0.6952\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6755 - acc: 0.6952\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.6722 - acc: 0.6952\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.6696 - acc: 0.6952\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6677 - acc: 0.6952\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6637 - acc: 0.6952\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.6610 - acc: 0.6952\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6587 - acc: 0.6952\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6562 - acc: 0.6952\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6534 - acc: 0.6952\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6509 - acc: 0.6952\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6488 - acc: 0.6952\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6461 - acc: 0.6952\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6433 - acc: 0.6952\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6412 - acc: 0.6952\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6388 - acc: 0.6952\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6372 - acc: 0.6952\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6348 - acc: 0.7048\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6324 - acc: 0.6952\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6301 - acc: 0.7048\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 38us/step - loss: 0.6281 - acc: 0.7048\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.6268 - acc: 0.7143\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6231 - acc: 0.7238\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6227 - acc: 0.7048\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6194 - acc: 0.7429\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6173 - acc: 0.7333\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6157 - acc: 0.7429\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6133 - acc: 0.7429\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6113 - acc: 0.7524\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6092 - acc: 0.7524\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6085 - acc: 0.7714\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6062 - acc: 0.7619\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.6032 - acc: 0.7524\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6033 - acc: 0.7619\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.6000 - acc: 0.7905\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5973 - acc: 0.7714\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5959 - acc: 0.7714\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5939 - acc: 0.7714\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.5924 - acc: 0.7714\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5906 - acc: 0.7810\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5889 - acc: 0.8095\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5867 - acc: 0.7810\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5860 - acc: 0.8095\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5834 - acc: 0.7905\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5814 - acc: 0.8000\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.5793 - acc: 0.7810\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.5775 - acc: 0.8190\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5770 - acc: 0.8000\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5810 - acc: 0.8190\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5730 - acc: 0.8095\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5712 - acc: 0.8095\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.5699 - acc: 0.8095\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.5675 - acc: 0.8286\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5659 - acc: 0.8190\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5656 - acc: 0.7905\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5632 - acc: 0.8476\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5615 - acc: 0.8476\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 114us/step - loss: 0.5598 - acc: 0.8286\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5611 - acc: 0.8381\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.5563 - acc: 0.8286\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.5554 - acc: 0.8381\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5529 - acc: 0.8476\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5517 - acc: 0.8571\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.5501 - acc: 0.8476\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.5257 - acc: 0.906 - 0s 76us/step - loss: 0.5485 - acc: 0.8476\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.5493 - acc: 0.8381\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5457 - acc: 0.8476\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5446 - acc: 0.8571\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.5429 - acc: 0.8667\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5427 - acc: 0.8762\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5411 - acc: 0.8762\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.5383 - acc: 0.8667\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.5372 - acc: 0.8476\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.5344 - acc: 0.8571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2eb9b52a0f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Model Prediction\n",
    "\n",
    "#fitting the model and predicting \n",
    "model.fit(X_train,Y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_test_class = np.argmax(Y_test,axis=1)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90        14\n",
      "           1       0.90      0.50      0.64        18\n",
      "           2       0.67      0.92      0.77        13\n",
      "\n",
      "   micro avg       0.78      0.78      0.78        45\n",
      "   macro avg       0.80      0.81      0.77        45\n",
      "weighted avg       0.81      0.78      0.76        45\n",
      "\n",
      "[[14  0  0]\n",
      " [ 3  9  6]\n",
      " [ 0  1 12]]\n"
     ]
    }
   ],
   "source": [
    "#y_predict=model.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test_class,y_pred_class))\n",
    "#Accuracy of the predicted values\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test_class,y_pred_class))\n",
    "print(confusion_matrix(y_test_class,y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P32ASP1Vjt0a"
   },
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8rd0jjAjyTR"
   },
   "outputs": [],
   "source": [
    "model.save('mnist_lc.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XiipRpe7rbVh"
   },
   "source": [
    "### Build and Train a Deep Neural network with 2 hidden layer  - Optional - For Practice\n",
    "\n",
    "Does it perform better than Linear Classifier? What could be the reason for difference in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5Du3lubr4sA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3. R6_InternalLab_AIML_Share_Prices-Eager Execution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
